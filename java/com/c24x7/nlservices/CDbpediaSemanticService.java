/*
 * Copyright (C) 2010-2012 Patrick Nicolas
 */
package com.c24x7.nlservices;

import java.io.IOException;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.util.LinkedList;
import java.util.List;

import com.c24x7.exception.SemanticAnalysisException;
import com.c24x7.models.ATaxonomyNode;
import com.c24x7.models.CText;
import com.c24x7.models.taxonomy.CTaxonomyModel;
import com.c24x7.models.taxonomy.CTaxonomyModel.NLikelihood;
import com.c24x7.semantics.CTaxonomyConnectionsPool;
import com.c24x7.semantics.CTaxonomyConnectionsPool.NTaxonomiesConn;
import com.c24x7.semantics.CTaxonomyExtractor;
import com.c24x7.semantics.dbpedia.CDbpediaSql;
import com.c24x7.textanalyzer.CTaggedNGramsExtractor;
import com.c24x7.textanalyzer.filters.CStopsFilter;
import com.c24x7.textanalyzer.stemmer.CPluralStemmer;
import com.c24x7.util.CEnv;
import com.c24x7.util.CFileUtil;
import com.c24x7.util.db.CSqlPreparedStmt;
import com.c24x7.util.logs.CLogger;
import com.c24x7.util.string.CStringUtil;



public class CDbpediaSemanticService implements Runnable {
	private static final String PREPARED_STATEMENT 				= "SELECT label,wnet,lgabstract,categories FROM 24x7c.dbpedia WHERE id=?;";
	private static final String ADD_TAXONOMY_PREPARED_STATEMENT = "UPDATE 24x7c.dbpedia SET wordnet = ? WHERE id=?;";
	
	private static final String TAXONOMY_DUMP 					= CEnv.debugDir + "taxonomy_dump";
	private static final int 	DISPLAY_FREQUENCY				= 500;

	private CSqlPreparedStmt	_pStmt 			= null;
	private CSqlPreparedStmt	_pUpdateStmt 	= null;
	private NTaxonomiesConn		_taxonomyCon	= null;
	private	boolean				_excludeTitle	= false;
	private	StringBuilder		_results		= null;
	private	int 				_startIndex		= -1;
	private	int					_endIndex		= -1;

	
	public CDbpediaSemanticService() throws SemanticAnalysisException {
		_taxonomyCon = CTaxonomyConnectionsPool.getInstance().getLabelsAndCatsConnection();
	}
	
	public void excludeTitle() {
		_excludeTitle = true;
	}
	
	/**
	 * <p>Turn the dump of all taxonomy lineages generated by this thread.</p>
	 */
	public void setDebug() {
		_results = new StringBuilder();
	}

		
	
	/**
	 * <p>Generate the taxonomy lineages for all the of Wikipedia entries as defined
	 * by their unique identified in the reference database. The taxonomy lineages are 
	 * generated from the  WordNet hypernyms, and Wikipedia long abstracts, labels and categories.</p>
	 * @throw IllegalArgumentException if the index are improperly defined.
	 */
	public void generate() {
		this.generate(1, (int)CDbpediaSql.getInstance().getNumEntries() - 1);
	}
	

	
	
	/**
	 * <p>Generate the taxonomy lineages of Wikipedia entries as defined
	 * by their unique identified in the reference database. The taxonomy lineages are 
	 * generated from the  WordNet hypernyms, and Wikipedia long abstracts, labels and categories.
	 * The generation starts at specific id in the DBpedia table.</p>
	 * @param startIndex index of the first Wikipedia record or row for which the taxonomy lineage has to be created.
	 * @throw IllegalArgumentException if the index are improperly defined.
	 */
	public void generate(int startIndex) {
		this.generate(startIndex, (int)CDbpediaSql.getInstance().getNumEntries() - 1);
	}

	
	
	
	/**
	 * <p>Generate the taxonomy lineages for a range of Wikipedia entries as defined
	 * by their unique identified in the reference database. The taxonomy lineages are 
	 * generated from the  WordNet hypernyms, and Wikipedia long abstracts, labels and categories.</p>
	 * @param startIndex index of the first Wikipedia record or row for which the taxonomy lineage has to be created.
	 * @param endIndex index of the last Wikipedia record or row (inclusive) for which the taxonomy lineage has to be created.
	 * @throw IllegalArgumentException if the index are improperly defined.
	 */
	public void generate(int startIndex, int endIndex) {
		if( endIndex <= startIndex) {
			throw new IllegalArgumentException("Incorrect table index for generating Taxonomy");
		}
		_startIndex = startIndex;
		_endIndex = endIndex;
		
		setDBConnections();
		new Thread(this).start();
	}
		

	
			/**
			 * <p>Coroutine implementing the extraction of taxonomy classes 
			 * (WordNet hypernyms) from Wikipedia abstract and generate the 
			 * taxonomy lineage associated to the label and abstract.
			 */
	public void run() {
		String taxonomyLineage = null;
		
		System.out.println("Start thread at " + _startIndex);
		for( int id = _startIndex; id <= _endIndex; id++) {
			try {
				taxonomyLineage = extractTaxonomy(id);
				if( taxonomyLineage != null) {
					writeTaxonomy(taxonomyLineage, id);
				}
				if(id % DISPLAY_FREQUENCY == 0) {
					System.out.println(id);
					try {
						CFileUtil.write(TAXONOMY_DUMP + String.valueOf(_startIndex), String.valueOf(id));
					}
					catch( IOException e) {
						CLogger.error(e.toString());
					}
				}
			}
			catch( SQLException e) {
				CLogger.error("Failed to add taxonomy to Dbpedia " + e.toString());
			}
		}
		
			/*
			 * If results have to be collected...
			 */
		if( _results != null ) {
			try {
				CFileUtil.write(TAXONOMY_DUMP, _results.toString());
			}
			catch( IOException e) {
				CLogger.error("Cannot store dump of taxonomy generation");
			}
		}
		
			/*
			 * Do not leave dangling sockets connection to Database.
			 */
		closeDbConnections();
	}
	
	
	
	public CText extract(final String inputText, final String label, final List<String> categoriesList) {
		List<String> keywordsAndStems = extractKeywordsAndStems(categoriesList);
		CText document = new CText(label);
			/*
			 * First extracts the N-Grams
			 */
		CTaggedNGramsExtractor nGramsExtractor = new CTaggedNGramsExtractor(true, _excludeTitle);
		if( nGramsExtractor.extract(document, inputText, keywordsAndStems) ) {
	
			/*
			 * then extract the taxonomy classes
			 */
			
			CTaxonomyExtractor taxonomyExtractor = new CTaxonomyExtractor(_taxonomyCon);
			if( taxonomyExtractor.extract(document) ) {
				document.setState(CText.E_STATES.TAXONOMY);	
			}
		}
		else {
			document.setState(CText.E_STATES.ERROR);
			document = null;
		}
		
		return document;
	}
	


	/**
	 * <p>Method to predict the taxonomy lineages or array of taxonomy classes 
	 * @param label  label or entry in the Wikipedia reference database
	 * @param lgAbstract long abstract associated with the label in the Wikipedia reference database
	 * @param categoriesList List of categories associated with the label in the Wikipedia reference database
	 * @param maxLikelihoodObj object to collect the parameters of the most relevant taxonomy classes.
	 * @return the most relevant taxonomy classes for this Wikipedia label and abstract.
	 */
	public ATaxonomyNode[] getRelevantClasses(	final String 		label, 
												final String 		lgAbstract, 
												final List<String> 	categoriesList,
												NLikelihood 		maxLikelihoodObj) {
		
		ATaxonomyNode[] bestTaxonomyLineage = null;

		/*
		 * Extract the model document associated to this Wikipedia label, long abstract
		 * and list of categories.
		 */
		CText document = extract(lgAbstract, label, categoriesList);	
	
		if( document != null) {				
			List<ATaxonomyNode[]> taxonomyClassesList = document.getTaxonomyClassesList();
	
				/*
				 * If the categories has been extracted from the Wikipedia reference
				 * database, then generate the associated statistics.
				 */
			if( taxonomyClassesList.size() > 1 ) {
				double 	likelihood = 0.0F,
					 	maxLikelihood = -Double.MAX_VALUE;
				
				CTaxonomyModel wnet = CTaxonomyModel.getInstance();
	
						/*
						 * Extract the the most relevant or likely set of taxonomy
						 * classes using the Taxonomy classifier.
						 */
				NLikelihood likelihoodObj = null;
				int maxClassId 	= -1, 
					classId		= -1;
				
				for(ATaxonomyNode[] taxonomyClasses :  taxonomyClassesList) {
					likelihoodObj = wnet.computeLikelihood(taxonomyClasses);
											
					likelihood = likelihoodObj.getLikelihood();
					classId = likelihoodObj.getClassId();
					
					if( maxLikelihood < likelihood) {
						bestTaxonomyLineage = taxonomyClasses;
						maxLikelihood = likelihood;
						maxClassId = classId;
					}
				}
				
				/*
				 * If the maximum likelihood has been computed, update the
				 * parameters of the results object.
				 */
				if( maxLikelihoodObj != null) {
					maxLikelihoodObj.set(maxClassId, maxLikelihood);
				}
			}
		}
			
		return bestTaxonomyLineage;
	}

	
						// ----------------------------
						//  Private Supporting Methods
						// -----------------------------
	
	private String extractTaxonomy(int id) throws SQLException {		
		String taxonomyLineageStr = null;
		
		_pStmt.set(1, id);		
		ResultSet rs = _pStmt.query();
		int wordnetHypernyms = -1;
		String categoriesStr = null;
		String lgAbstract = null;
		String keyword = null;

		if( rs.next() ) {
			keyword = rs.getString("label");
			wordnetHypernyms = rs.getInt("wnet");
			categoriesStr = rs.getString("categories");
			lgAbstract = rs.getString("lgabstract");
		}
		
			/*
			 * We only consider the Wikipedia records which do not have a taxonomy lineage
			 */
		if(wordnetHypernyms == 0 ) {
			final String decodedKeyword = CStringUtil.decodeLatin1(keyword);
			final String decodedLgAbstract = CStringUtil.decodeLatin1(lgAbstract);
		
			if( decodedLgAbstract != null && decodedLgAbstract.length() > CTaxonomyModel.MIN_VALID_CONTENT_LENGTH ) {
	
				List<String> categoriesList = null;
				
				if( CTaxonomyModel.getInstance().hasCategories() ) {
					categoriesList = new LinkedList<String>();
					
					final String decodedCategoryStr = CStringUtil.decodeLatin1(categoriesStr, CEnv.ENCODED_ENTRY_FIELDS_DELIM);
					
					if( decodedCategoryStr != null ) {
						String[] categoriesArray = decodedCategoryStr.split(CEnv.ENTRY_FIELDS_DELIM); 
							
							/*
							 * collect all categories without duplicating the label which
							 * may be included in the categories list.
							 */
						for( String category : categoriesArray) {
							if( category.length() > 2) {
								categoriesList.add(category);
							}
						}
					}
				}
				
				ATaxonomyNode[] taxonomyLineage = getRelevantClasses(decodedKeyword, decodedLgAbstract, categoriesList, null);
				
				if( taxonomyLineage != null) {
					taxonomyLineageStr = CTaxonomyModel.convertClassesToLineage(taxonomyLineage);
					StringBuilder buf = new StringBuilder(taxonomyLineageStr);
					buf.append("/");
					buf.append(decodedKeyword);
					
					taxonomyLineageStr = buf.toString();
					
					if( _results != null ) {
						_results.append("\n\n");
						_results.append(decodedKeyword);
						_results.append("\n");
						_results.append(decodedLgAbstract);
						_results.append("\n\n=>");
						_results.append(taxonomyLineageStr);
					}
				}
			}
		}
		
		return taxonomyLineageStr;
	}
	
	

	private void writeTaxonomy(final String taxonomyStr, int id) throws SQLException {
		
		final String encodedTaxonomyStr = CStringUtil.encodeLatin1(taxonomyStr);
		_pUpdateStmt.set(1, encodedTaxonomyStr);
		_pUpdateStmt.set(2, id);
		
		_pUpdateStmt.update();
	}
	


	private void setDBConnections() {
		_pStmt = new CSqlPreparedStmt(PREPARED_STATEMENT);
		_pUpdateStmt =  new CSqlPreparedStmt(ADD_TAXONOMY_PREPARED_STATEMENT);
	}
	
	
	private void closeDbConnections() {
		if( _pStmt != null ) {
			_pStmt.close();
		}
		if( _pUpdateStmt != null ) {
			_pUpdateStmt.close();
		}
	}

	

	private static List<String> extractKeywordsAndStems(final List<String> keywordsList) {
		
		List<String> keywordsAndStems = null;
		
		if( keywordsList != null) {
			keywordsAndStems = new LinkedList<String>();
			
			String[] terms = null;
			String stem = null;
			String stopWord = null;
			
			for(String keyword : keywordsList) {
			
				keywordsAndStems.add(keyword);
				terms = keyword.split(" ");
				stem = null;
				
				/*
				 * If the category has multiple terms..
				 */
				if( terms != null && terms.length > 1) {
					stem = CPluralStemmer.getInstance().stem(terms[terms.length-1]);
						
					if(stem != null) {
						StringBuilder buf = new StringBuilder();
						for(int k = 0; k < terms.length-1; k++) {
							buf.append(terms[k]);
							buf.append(" ");
						}
						buf.append(stem);
						keywordsAndStems.add(buf.toString());
					}
					
					for( int k = 0; k < terms.length; k++) {
						stopWord = CStopsFilter.getInstance().qualify(terms[k]);
						
						if( stopWord != null) {
							keywordsAndStems.add(stopWord);
						}
					}
				}
				else {
					stem = CPluralStemmer.getInstance().stem(keyword);
					if( stem != null) {
						keywordsAndStems.add(stem);
					}
				}
			}
		}
		
		return keywordsAndStems;
	}
}
